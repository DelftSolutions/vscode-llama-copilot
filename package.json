{
	"name": "llama-copilot",
	"publisher": "delft-solutions",
	"displayName": "Copilot for llama-server LLMs",
	"description": "VS Code extension that integrates llama-server LLMs as language model chat providers.",
	"version": "1.1.0",
	"icon": "llama1-icon.png",
	"license": "MIT",
	"repository": {
		"type": "git",
		"url": "https://github.com/DelftSolutions/vscode-llama-copilot"
	},
	"engines": {
		"vscode": "^1.104.0"
	},
	"activationEvents": [
		"onLanguageModelAccess:llama-server",
		"onStartupFinished"
	],
	"categories": [
		"AI",
		"Chat"
	],
	"contributes": {
		"languageModelChatProviders": [
			{
				"vendor": "llama-server",
				"displayName": "LLaMA Server",
				"managementCommand": "llamaCopilot.openEndpointSettings"
			}
		],
		"commands": [
			{
				"command": "llamaCopilot.openEndpointSettings",
				"title": "Open Endpoint Settings"
			}
		],
		"configuration": {
			"title": "LLaMA Server",
			"properties": {
				"llamaCopilot.endpoints": {
					"type": "object",
					"description": "Configuration for llama-server endpoints. Each key is an endpoint identifier, and models will be suffixed with @<identifier> (e.g., model-name@local)",
					"additionalProperties": {
						"type": "object",
						"properties": {
							"url": {
								"type": "string",
								"description": "URL of the llama-server instance (e.g., http://localhost:8013)"
							},
							"apiToken": {
								"type": "string",
								"description": "Optional API token for authenticated endpoints"
							},
							"headers": {
								"type": "object",
								"description": "Optional headers to add to all requests for this endpoint",
								"additionalProperties": {
									"type": "string"
								}
							},
							"requestBody": {
								"type": "object",
								"description": "Optional JSON properties to merge into request body for this endpoint"
							},
							"models": {
								"type": "object",
								"description": "Per-model configuration. Each key is a model ID (without @endpoint suffix)",
								"additionalProperties": {
									"type": "object",
									"properties": {
										"headers": {
											"type": "object",
											"description": "Optional headers specific to this model (merged with endpoint headers, model headers take precedence)",
											"additionalProperties": {
												"type": "string"
											}
										},
										"requestBody": {
											"type": "object",
											"description": "Optional JSON properties to merge into request body for this model (merged with endpoint requestBody, model properties take precedence)"
										},
										"contextSize": {
											"type": "number",
											"description": "Override context size for this model (in tokens)"
										},
										"maxOutputTokens": {
											"type": "number",
											"description": "Override maximum output tokens for this model"
										},
										"capabilities": {
											"type": "object",
											"description": "Override capabilities for this model",
											"properties": {
												"imageInput": {
													"type": "boolean",
													"description": "Whether image input is supported by the model"
												},
												"toolCalling": {
													"description": "Whether tool calling is supported. Can be a boolean or a number (maximum number of tools)",
													"oneOf": [
														{
															"type": "boolean"
														},
														{
															"type": "number"
														}
													]
												}
											}
										}
									}
								}
							}
						},
						"required": [
							"url"
						]
					}
				},
				"llamaCopilot.requestTimeoutSeconds": {
					"type": "number",
					"default": 3600,
					"description": "Request timeout in seconds for llama-server (headers and body). Increase for large contexts."
				},
				"llamaCopilot.enableCursorRules": {
					"type": "boolean",
					"default": true,
					"description": "Enable cursor rules tool for LLM access to project rules from .cursor/rules/"
				},
				"llamaCopilot.inlineCompletionModel": {
					"type": "string",
					"default": "",
					"description": "Model ID for inline (ghost) completions in the editor (e.g. sweep-next-edit-1.5b@local). If empty, inline completions are disabled."
				},
				"llamaCopilot.inlineCompletionMaxInputBytes": {
					"type": "number",
					"default": 16384,
					"description": "Maximum total input size in bytes (UTF-8) for inline completion requests (prefix + suffix + context)."
				},
				"llamaCopilot.inlineCompletionTimeoutMs": {
					"type": "number",
					"default": 30000,
					"description": "Request timeout in milliseconds for inline completion requests. No suggestion is shown on timeout."
				},
				"llamaCopilot.inlineCompletionDebounceMs": {
					"type": "number",
					"default": 2000,
					"description": "Debounce delay in milliseconds before sending an automatic (as-you-type) inline completion request. Invoked completions are not debounced."
				},
				"llamaCopilot.inlineCompletionIncludeContext": {
					"type": "boolean",
					"default": true,
					"description": "Use open tab contents to provide better inline completions."
				},
				"llamaCopilot.debug.modelListFetch": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for model list fetch operations"
				},
				"llamaCopilot.debug.completion": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for completion requests"
				},
				"llamaCopilot.debug.tokenization": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for tokenization operations"
				},
				"llamaCopilot.debug.rulesMatching": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for rules matching operations"
				},
				"llamaCopilot.debug.toolCalls": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for tool calls"
				},
				"llamaCopilot.debug.inlineCompletion": {
					"type": "boolean",
					"default": false,
					"description": "Enable debug logging for inline completion requests, cancellations, and errors"
				}
			}
		}
	},
	"main": "./out/extension.js",
	"scripts": {
		"vscode:prepublish": "npm run compile",
		"download-api": "dts dev",
		"postdownload-api": "dts main",
		"postinstall": "npm run download-api",
		"compile": "tsc -p ./",
		"test": "vitest run",
		"lint": "eslint",
		"watch": "tsc -watch -p ./"
	},
	"dependencies": {
		"undici": "^6.21.0"
	},
	"devDependencies": {
		"@eslint/js": "^9.13.0",
		"@stylistic/eslint-plugin": "^2.9.0",
		"@types/node": "^22",
		"@types/vscode": "^1.104.0",
		"@vscode/dts": "^0.4.1",
		"@vscode/vsce": "^3.7.1",
		"eslint": "^9.13.0",
		"typescript": "^5.9.2",
		"typescript-eslint": "^8.39.0",
		"vitest": "^4.0.18"
	}
}
