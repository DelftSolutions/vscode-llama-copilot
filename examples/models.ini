version = 1

; Made for Macbook Pro with 128GB RAM

; Use lower context sizes for lower RAM usage
; 1.000.000 context size requires roughly 133GB RAM
;   256.000 context size requires roughly 33GB RAM
;   128.000 context size requires roughly 17GB RAM
;    32.768 context size requires roughly 4GB RAM

; Use smaller quantizations for lower RAM usage
; For example with a 30b model:
; BF16 => 2 * 30GB = 60GB RAM
; Q8_0 => 1 * 30GB = 30GB RAM
; Q4_0 => 0.5 * 30GB = 15GB RAM
; Q1_0 => 0.125 * 30GB = 3.75GB RAM

[glm-4.5-air-5bit]
jinja = true
ctx-size = 0
temp = 0.6
top-p = 0.95
fit = on
hf = unsloth/GLM-4.5-AIR-GGUF:Q5_K_XL

[bge-reranker-v2-m3-q8]
ctx-size = 0
ubatch-size = 32768
batch-size = 32768
reranking = true
hf = gpustack/bge-reranker-v2-m3-GGUF:Q8_0
load-on-startup = true

[bge-m3]
ctx-size = 8192
ubatch-size = 8192
batch-size = 8192
embedding = true
model = models/bge-m3.gguf
load-on-startup = true

[gemma3n]
jinja = true
ctx-size = 0
temp = 1.0
top-k = 64
min-p = 0.01
top-p = 0.95
repeat-penalty = 1.0
fit = on
hf = unsloth/gemma-3-27b-it-qat-GGUF:Q4_K_XL
stop-timeout = 120

[nemotron-3-nano-30b]
jinja = true
ctx-size = 256000
temp = 1.0
top-p = 1.00
fit = on
hf = unsloth/Nemotron-3-Nano-30B-A3B-GGUF:BF16
stop-timeout = 120

[qwen3-4b]
jinja = true
ctx-size = 32768
temp = 0.6
min-p = 0.0
top-p = 0.95
top-k = 20
hf = unsloth/Qwen3-4B-128K-GGUF:Q8_K_XL
load-on-startup = true

[glm-4-7-flash]
jinja = true
ctx-size = 202752
temp = 0.7
top-p = 1.0
min-p = 0.01
repeat-penalty = 1.0
hf = unsloth/GLM-4.7-Flash-GGUF:BF16
stop-timeout = 120

